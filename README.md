This repository contains the project done for Cognitive, Behavioral and social data course. 

Project participiants:

1. ALEJANDRA OLIVIA CRUCES ANDREWS
2. CHELSIE ADELLE RENESSA ROMAIN
3. MARIO ALEJANDRO TAPIA MONTERO
4. ROVENA LLAPUSHI

DATASET:

This dataset is being used in scope of the course project. 


Reference: Capuozzo, P., Lauriola, I., Strapparava, C., Aiolli, F., & Sartori, G. DecOp: A multilingual and multi-
domain corpus for detecting deception in typed text. In Proceedings of the 12th Language Resources and Evaluation Conference, 1423-1430, (2020, May)



Task ideas, summarizing what is already written in the notebook (to be updates, based on prof feedback):

1. add a summary describing the datset and the problem (what is written in the paper)
2. do a diagram of how GPT-X works
3. understand better the model behaviour when uring Trainer method from hugging face (ask the prof if the methodology is right)  Describe the methodology and motivate the choice  ----------------------> assigned to: Rovena
   
       3.1 try with smaller model (distil-gpt2)
   
4. we can implement our own train and evaluate method: follow the instructions given here:   https://colab.research.google.com/github/gmihaila/ml_things/blob/master/notebooks/pytorch/gpt2_finetune_classification.ipynb

After we get feedback:

1. try to run the model available with different hyperparameters (wait for point 3)

[

Mario: 1e-5,
Chelsie: 2e-5,
Rovena: 3e-5
Alejandra: to be defined
]


2. if the cross validation strategy changes and we still need to use API:
   
    2.1 understand how to use and understand the response of openai API
   
         a. OPENAI notebook:  https://github.com/openai/openai-cookbook/blob/main/examples/Fine-tuned_classification.ipynb
         b. prof link: https://www.ignorance.ai/p/how-to-fine-tune-chatgpt?r=92azo&utm_medium=ios&utm_campaign=post
         c. useful resources: 
            https://www.datacamp.com/tutorial/fine-tuning-gpt-3-using-the-open-ai-api-and-python
            https://www.datacamp.com/tutorial/how-to-fine-tune-gpt3-5
   
 4. error analysis

    
 5. cognitive interpretation
   
